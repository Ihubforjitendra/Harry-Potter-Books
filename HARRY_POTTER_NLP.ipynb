{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f8b7c58",
   "metadata": {},
   "source": [
    "   # Turn a Harry Potter Book into a Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacf864f",
   "metadata": {},
   "source": [
    "  ## Developed_By_Jitendra_And_Suggested_From_NEO4J."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a644f",
   "metadata": {},
   "source": [
    "Learn how to combine Selenium and SpaCy to create a Neo4j knowledge graph of the Harry Potter universe.\n",
    "        Visite For More Info About Harry Potter Books : https://harrypotter.fandom.com/wiki/Main_Page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a9572",
   "metadata": {},
   "source": [
    "First Things is That you have to install the required dependencies for this project Available Inside requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2694fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup selenium\n",
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "wd = webdriver.Chrome(chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f410a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_single_item(item):\n",
    "  try:\n",
    "    # Find the HTML element with required data\n",
    "    div = wd.find_element_by_xpath(f\"//div[@data-source = '{item}']\")\n",
    "    # Extract relevant data from \"a\" or \"div\" tag\n",
    "    try:\n",
    "      result = div.find_element(By.TAG_NAME, \"a\").text.split('[')[0].strip()\n",
    "      if result[0] == '[':\n",
    "        raise Exception\n",
    "    except:\n",
    "      result = div.find_element(By.TAG_NAME, \"div\").text.split('[')[0].strip()\n",
    "    return result\n",
    "  except:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231db199",
   "metadata": {},
   "source": [
    "                                Harry Potter fandom page scraping\n",
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "We will use Selenium for web scraping. As mentioned, we will begin by scraping the characters in the Harry Potter and the Philosopher's Stone book. The list of characters by chapter is available under the CC-BY-SA license, so we don't have to worry about any copyright infringement. Additionally, each of the characters has a web page with detailed information about the character. For example, if you check out the Hermione Granger page, you can observe a structured table with additional information. We will use the alias section for the entity extraction and add other character details like house and blood type to enrich our knowledge graph. visite for info : https://harrypotter.fandom.com/wiki/Main_Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf232782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_characters(url):\n",
    "  # Get the list of characters by chapter\n",
    "  wd.get(url)\n",
    "  character_dict = dict()\n",
    "  elem = wd.find_element(By.CLASS_NAME,  \"mw-parser-output\")\n",
    "  \n",
    "  # Locate character by chapter\n",
    "  tables = elem.find_elements(By.TAG_NAME, 'table')\n",
    "  for i, chapter in enumerate(tables):\n",
    "    list_of_characters = []\n",
    "    characters = chapter.find_elements(By.TAG_NAME, 'a')\n",
    "    for character in characters:\n",
    "      if not character.get_attribute('title'):\n",
    "        continue\n",
    "      list_of_characters.append({'title': character.get_attribute('title'), 'url': character.get_attribute('href')})\n",
    "    character_dict['chapter_' + str(i + 1)] = list_of_characters\n",
    "  # Enrich characters with additional information\n",
    "  for chapter in character_dict:\n",
    "    for index, character in enumerate(character_dict[chapter]):\n",
    "      # Rate limit sleep\n",
    "      time.sleep(1)\n",
    "      # Get the character page with selenium\n",
    "      wd.get(character['url'])\n",
    "      # Enrich aliases\n",
    "      try:\n",
    "        alias_div = wd.find_element_by_xpath(\"//div[@data-source = 'alias']\")\n",
    "        aliases = alias_div.find_elements(By.TAG_NAME, 'li')\n",
    "        result = []\n",
    "        for a in aliases:\n",
    "          # Ignore under the cloak-guise and the name he told\n",
    "          if \"disguise\" in a.text or \"the name he told\" in a.text:\n",
    "            continue\n",
    "          alias = a.text.split('[')[0].split('(')[0].strip()\n",
    "          result.append(alias)\n",
    "        character_dict[chapter][index]['aliases'] = result\n",
    "      except:\n",
    "        pass\n",
    "      # Enrich loyalties\n",
    "      try:\n",
    "        loyalty_div = wd.find_element_by_xpath(\"//div[@data-source = 'loyalty']\")\n",
    "        loyalties = loyalty_div.find_elements(By.TAG_NAME, 'li')\n",
    "        result = []\n",
    "        for l in loyalties:\n",
    "          loyalty = l.text.split('[')[0].split('(')[0].strip()\n",
    "          result.append(loyalty)\n",
    "        character_dict[chapter][index]['loyalty'] = result\n",
    "      except:\n",
    "        pass\n",
    "      # Enrich family relationships\n",
    "      try:\n",
    "        family_div = wd.find_element_by_xpath(\"//div[@data-source = 'family']\")\n",
    "        relationships = family_div.find_elements(By.TAG_NAME, 'li')\n",
    "        result = []\n",
    "        for r in relationships:\n",
    "          rel = r.text.split('[')[0].split('(')[0].strip()\n",
    "          rel_type = r.text.split('(')[-1].split(')')[0].split('[')[0]\n",
    "          result.append({'person':rel, 'type': rel_type})\n",
    "        character_dict[chapter][index]['family'] = result\n",
    "      except:\n",
    "        pass\n",
    "      # Enrich blood\n",
    "      character_dict[chapter][index]['blood'] = enrich_single_item('blood')\n",
    "      # Enrich nationality\n",
    "      character_dict[chapter][index]['nationality'] = enrich_single_item('nationality')\n",
    "      # Enrich species\n",
    "      character_dict[chapter][index]['species'] = enrich_single_item('species')\n",
    "      # Enrich house\n",
    "      character_dict[chapter][index]['house'] = enrich_single_item('house')\n",
    "      # Enrich gender\n",
    "      character_dict[chapter][index]['gender'] = enrich_single_item('gender')\n",
    "  return character_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62ca22e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_dict = get_characters(\"https://harrypotter.fandom.com/wiki/Harry_Potter_and_the_Philosopher%27s_Stone_(character_index)\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e57dfa42",
   "metadata": {},
   "source": [
    "Before we continue with named entity extraction from the book, we will store the scraped information about the characters to Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1fc2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "# Change the host and user/password combination to your neo4j\n",
    "# Will not work with a localhost bolt url\n",
    "host = 'bolt://54.92.169.42:7687'\n",
    "user = 'neo4j'\n",
    "password = 'dereliction-hisses-manuals'\n",
    "driver = GraphDatabase.driver(host,auth=(user, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f7696af",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_query = \"\"\"\n",
    "UNWIND $data as row\n",
    "MERGE (c:Character{name:row.title})\n",
    "SET c.url = row.url,\n",
    "    c.aliases = row.aliases,\n",
    "    c.blood = row.blood,\n",
    "    c.nationality = row.nationality,\n",
    "    c.species = row.species,\n",
    "    c.gender = row.gender\n",
    "FOREACH (h in CASE WHEN row.house IS NOT NULL THEN [1] ELSE [] END | MERGE (h1:House{name:row.house}) MERGE (c)-[:BELONGS_TO]->(h1))\n",
    "FOREACH (l in row.loyalty | MERGE (g:Group{name:l}) MERGE (c)-[:LOYAL_TO]->(g))\n",
    "FOREACH (f in row.family | MERGE (f1:Character{name:f.person}) MERGE (c)-[t:FAMILY_MEMBER]->(f1) SET t.type = f.type)    \n",
    "\n",
    "\"\"\"\n",
    "with driver.session() as session:\n",
    "  for chapter in character_dict:\n",
    "    session.run(entity_query, {'data': character_dict[chapter]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f74eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_character_dict(chapter):\n",
    "  super_list = list()\n",
    "  dicts = [character_dict['chapter_' + str(i)] for i in range(1,chapter + 1)]\n",
    "  for d in dicts:\n",
    "    for item in d:\n",
    "      super_list.append(item)\n",
    "  return super_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3819ac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jiten\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.9). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[My sister: [My sister, She], a dog: [a dog, him]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import neuralcoref\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "doc1 = nlp('My sister has a dog. She loves him.')\n",
    "print(doc1._.coref_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2eed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coref_resolution(text):\n",
    "    \"\"\"Function that executes coreference resolution on a given text\"\"\"\n",
    "    doc = nlp(text)\n",
    "    # fetches tokens with whitespaces from spacy document\n",
    "    tok_list = list(token.text_with_ws for token in doc)\n",
    "    for cluster in doc._.coref_clusters:\n",
    "        # get tokens from representative cluster name\n",
    "        cluster_main_words = set(cluster.main.text.split(' '))\n",
    "        for coref in cluster:\n",
    "            if coref != cluster.main:  # if coreference element is not the representative element of that cluster\n",
    "                if coref.text != cluster.main.text and bool(set(coref.text.split(' ')).intersection(cluster_main_words)) == False:\n",
    "                    # if coreference element text and representative element text are not equal and none of the coreference element words are in representative element. This was done to handle nested coreference scenarios\n",
    "                    tok_list[coref.start] = cluster.main.text + \\\n",
    "                        doc[coref.end-1].whitespace_\n",
    "                    for i in range(coref.start+1, coref.end):\n",
    "                        tok_list[i] = \"\"\n",
    "\n",
    "    return \"\".join(tok_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac86eb3",
   "metadata": {},
   "source": [
    "                    Entity recognition with SpaCy's rule-based matching\n",
    "                ==========================================================\n",
    "First, I wanted to be cool and use a Named Entity Recognition model. I've tried models from SpaCy, HuggingFace, Flair, and even Stanford NLP. None of them worked well enough to satisfy my requirements. So instead of training my model, I decided to use SpaCy's rule-based pattern matching feature. We already know which characters we are looking for based on the data we scraped from the HP fandom site. Now we just have to find a way to match them in the text as perfectly as possible. We have to define the text patterns for each of the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efdbfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matcher_patterns(character):\n",
    "  matcher_pattern = []\n",
    "  stop_words = ['of', 'the', 'at', 'family', 'keeper', 'wizard', 'fat', 'de', 'hogwarts', 'hotel', 'owner', 'express']\n",
    "  parts_of_name = [el for el in character['title'].split(' ') if len(el) > 2]\n",
    "  # Append the whole pattern\n",
    "  matcher_pattern.append([{\"LOWER\": n.lower(), \"IS_TITLE\": True} for n in parts_of_name])\n",
    "  \n",
    "  # Append parts of names\n",
    "  if not \"'\" in character['title']: # Skip names like Vernon Dursley's secretary\n",
    "    for n in parts_of_name:\n",
    "      if n.lower() in stop_words: # Skip appending stop words\n",
    "        continue\n",
    "      matcher_pattern.append([{\"LOWER\": n.lower(), \"IS_TITLE\": True}])\n",
    "      # Special case for Ronald Weasley -> Also add Ron\n",
    "      if n == \"Ronald\":\n",
    "        matcher_pattern.append([{\"LOWER\": \"ron\", \"IS_TITLE\": True}])\n",
    "  return matcher_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8076817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "hardcoded_options = dict()\n",
    "hardcoded_options['Malfoy'] = ['Draco Malfoy']\n",
    "hardcoded_options['Patil'] = ['Padma Patil', 'Parvati Patil']\n",
    "hardcoded_options['Tom'] = ['Tom']\n",
    "\n",
    "def handle_multiple_options(result, doc):\n",
    "  needs_deduplication = [(i,x) for i,x in enumerate(result) if len(x['string_id']) > 1]\n",
    "  for index, multiple_options in needs_deduplication:\n",
    "    # Special logic for Dursleys, if there if Mr. then Vernon, if Mrs. then Petunia\n",
    "    prefix = doc[multiple_options['start']-3 : multiple_options['start']]\n",
    "    if (multiple_options['text'] == 'Dursley') and (\"Mr.\" in prefix.text):\n",
    "      resolution = [\"Vernon Dursley\"]\n",
    "    elif (multiple_options['text'] == 'Dursley') and (\"Mrs.\" in prefix.text):\n",
    "      resolution = [\"Petunia Dursley\"]\n",
    "    # Find nearest entity\n",
    "    else:\n",
    "      end_char = multiple_options['end']\n",
    "      distance = sys.maxsize\n",
    "      resolution = []\n",
    "      for possible_option in result:\n",
    "        # Skip multiple options and entities that don't have any of the multiple option\n",
    "        if (not len(possible_option['string_id']) == 1) or (not possible_option['string_id'][0] in multiple_options['string_id']):\n",
    "          continue\n",
    "        new_distance = abs(multiple_options['end'] - possible_option['end'])\n",
    "        if new_distance < distance:\n",
    "          distance = new_distance\n",
    "          resolution = possible_option['string_id']\n",
    "      \n",
    "      if not resolution:\n",
    "        try:\n",
    "          ho = hardcoded_options[multiple_options['text']]\n",
    "          if len(ho) == 1:\n",
    "            resolution = ho\n",
    "          else:\n",
    "            resolution = [random.choice(ho)]\n",
    "        except:\n",
    "          print(f\"no way to disambiguate {multiple_options['text']} from options: {multiple_options['string_id']}\")\n",
    "    \n",
    "    result[index]['string_id'] = resolution\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe503d",
   "metadata": {},
   "source": [
    "                             Infer relationships between characters\n",
    "                           ===========================================\n",
    "We are finished with the hard part. Inferring relationships between characters is very simple. First, we need to define the distance threshold of interaction or relation between two characters. As mentioned, we will use the same distance threshold as was used in the GoT extraction. That is, if two characters co-occur within the distance of 14 words, then we assume they must have interacted. I have also merged entities not to skew results. What do I mean by joining entities? Suppose we have the following two sentences: \"Harry was having a good day. He went to talk to Dumbledore in the afternoon.\" Our entity extraction process will identify three entities, \"Harry\", \"He\" as a reference to Harry, and \"Dumbledore\". If we took the naive approach, we could infer two interactions between Harry and Dumbledore as two references of \"Harry\" are close to \"Dumbledore\". However, I want to avoid that, so I have merged entities in a sequence that refers to the same character as a single entity. Finally, we have to count the number of interactions between the pairs of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "355616e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_distances(result, distance_threshold):\n",
    "  #sort by start character\n",
    "  result = sorted(result, key=lambda k: k['start'])\n",
    "  compact_entities = []\n",
    "  # Merge entities\n",
    "  for entity in result:\n",
    "    # If the same entity occurs, prolong the end \n",
    "    if (len(compact_entities) > 0) and (compact_entities[-1]['string_id'] == entity['string_id']):\n",
    "      compact_entities[-1]['end'] = entity['end']\n",
    "    else:\n",
    "      compact_entities.append(entity)\n",
    "  distances = list()\n",
    "  # Iterate over all entities\n",
    "  for index, source in enumerate(compact_entities[:-1]):\n",
    "    # Compare with entities that come after the given one\n",
    "    for target in compact_entities[index + 1:]:\n",
    "      if (source['string_id'] != target['string_id']) and (abs(source['end'] - target['start']) < distance_threshold):\n",
    "        link = sorted([source['string_id'][0], target['string_id'][0]])\n",
    "        distances.append(link)\n",
    "      else:\n",
    "        break\n",
    "  # Count the number of interactions\n",
    "  return Counter(map(tuple, distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b53b77",
   "metadata": {},
   "source": [
    "                               Store results to Neo4j graph database\n",
    "                            ===========================================\n",
    "We have extracted the interactions network between character, and the only thing left is to store the results into a graph database. The import query is very straightforward as we are dealing with a monopartite network. If you are using the Colab notebook I have prepared, then it would be easiest to create either a Neo4j Sandbox or Aura database instance to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3f370d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_to_neo4j(distances):\n",
    "  data = [{'source': el[0], 'target': el[1], 'weight': distances[el]} for el in distances]\n",
    "  with driver.session() as session:\n",
    "    session.run(\"\"\"\n",
    "    UNWIND $data as row\n",
    "    MERGE (c:Character{name:row.source})\n",
    "    MERGE (t:Character{name:row.target})\n",
    "    MERGE (c)-[i:INTERACTS]-(t)\n",
    "    SET i.weight = coalesce(i.weight,0) + row.weight\n",
    "    \"\"\", {'data': data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db67ea",
   "metadata": {},
   "source": [
    "First of all, we have to get our hands on the text from the book. I've found a GitHub repository that contains the text of the first four Harry Potter books. There is no license attached to the data, so I will assume we can use the data for educational purposes within the limits of fair use. If you actually want to read the book, please go and buy it. Getting the text from a GitHub file is quite easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c7b9a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get(\"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\")\n",
    "text = res.text\n",
    "chapters = text.split(\"CHAPTER\")[1:]\n",
    "def get_characters_in_chapter(chapter):\n",
    "  c = chapters[chapter - 1]\n",
    "  # Prepare characters matcher\n",
    "  matcher = Matcher(nlp.vocab)\n",
    "  for character in get_character_dict(chapter):\n",
    "    matcher_pattern = get_matcher_patterns(character)\n",
    "    matcher.add(character['title'], matcher_pattern)\n",
    "\n",
    "  # Prepare text\n",
    "  lines = c.split('\\n')[1:]\n",
    "  lines = list(filter(None, lines))\n",
    "  chapter_title = lines[0]\n",
    "  print(chapter_title)\n",
    "  text = \" \".join(lines[1:])\n",
    "  \n",
    "  # Run coreference resolution\n",
    "  text = coref_resolution(text)\n",
    "\n",
    "  # Find matches\n",
    "  doc = nlp(text)\n",
    "  matches = matcher(doc)\n",
    "  result = []\n",
    "  for match_id, start, end in matches:\n",
    "      string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "      span = doc[start:end]  # The matched span\n",
    "\n",
    "      # Get predicates for correct result appendment\n",
    "      exists_longer = [(start == e['start'] and end < e['end']) or (start > e['start'] and end == e['end']) for e in result]\n",
    "      same = [start == e['start'] and end == e['end'] for e in result]\n",
    "      shorter_end = [start == e['start'] and end > e['end'] for e in result]\n",
    "      shorter_start = [start < e['start'] and end == e['end'] for e in result]\n",
    "      \n",
    "      # Append to results\n",
    "      if any(exists_longer): # If there is a longer version of the given entity already in results\n",
    "        continue\n",
    "      \n",
    "      if any(shorter_end): # If there is any entity with the same start span but has shorter end\n",
    "        del result[shorter_end.index(True)]\n",
    "        result.append({'string_id': [string_id], 'start': start, 'end': end, 'text': span.text}) \n",
    "      elif any(shorter_start): # If there is any entity with the same end span but has shorter start\n",
    "        del result[shorter_start.index(True)]\n",
    "        result.append =({'string_id': [string_id], 'start': start, 'end': end, 'text': span.text}) \n",
    "      elif not any(same): # If not exists yet\n",
    "        result.append({'string_id': [string_id], 'start': start, 'end': end, 'text': span.text})\n",
    "      else: # Add more entities to a single span\n",
    "        i = same.index(True)\n",
    "        result[i]['string_id'].append(string_id)\n",
    "  \n",
    "  # Handle results where there are multiple options\n",
    "  handle_multiple_options(result, doc)\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7faf4a8",
   "metadata": {},
   "source": [
    "Run the code for each chapter of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6859f9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BOY WHO LIVED\n",
      "THE VANISHING GLASS\n",
      "THE LETTERS FROM NO ONE\n",
      "THE KEEPER OF THE KEYS\n",
      "DIAGON ALLEY\n",
      "THE JOURNEY FROM PLATFORM NINE AND THREE-QUARTERS\n",
      "THE SORTING HAT\n",
      "THE POTIONS MASTER\n",
      "THE MIDNIGHT DUEL\n",
      "HALLOWEEN\n",
      "QUIDDITCH\n",
      "THE MIRROR OF ERISED\n",
      "NICOLAS FLAMEL\n",
      "NORBERT THE NORWEGIAN RIDGEBACK\n",
      "THE FORIBIDDEN FOREST\n",
      "THROUGH THE TRAPDOOR\n",
      "THE MAN WITH TWO FACES\n"
     ]
    }
   ],
   "source": [
    "for c in range(1,len(chapters) + 1):\n",
    "  end = get_characters_in_chapter(c)\n",
    "  distances = get_distances(end, 14)\n",
    "  store_to_neo4j(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053f56a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
